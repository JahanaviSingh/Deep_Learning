{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a782c9e-f8be-4886-bd85-305564d56764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: RNN From Scratch ---\n",
      "Computed Hidden State Shape: (16, 1)\n",
      "NumPy Implementation logic complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Basic RNN Forward Pass Implementation\n",
    "def rnn_scratch_demo():\n",
    "    # Hyperparameters\n",
    "    input_size = 10   # Size of one-hot vector\n",
    "    hidden_size = 16  # Size of hidden state\n",
    "    \n",
    "    # Initialize weights with small random values\n",
    "    Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "    bh = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    # Initial states\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "    x_t = np.random.randn(input_size, 1) # Simulated input\n",
    "    \n",
    "    # Hidden state calculation: h_t = tanh(Wxh * x_t + Whh * h_{t-1} + b)\n",
    "    h_t = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h_prev) + bh)\n",
    "    \n",
    "    print(\"--- Part 1: RNN From Scratch ---\")\n",
    "    print(f\"Computed Hidden State Shape: {h_t.shape}\")\n",
    "    print(\"NumPy Implementation logic complete.\\n\")\n",
    "\n",
    "rnn_scratch_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930a335f-df2a-4a3d-9b81-7de75e1f1f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preprocessing ---\n",
      "Total Unique Words (Vocab): 6989\n",
      "Total Training Pairs: 24634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Load Data\n",
    "\n",
    "df = pd.read_csv('/Users/jahanavisingh/Downloads/poems-100.csv')\n",
    "\n",
    "# Extract 'text' column and drop any empty rows\n",
    "poems_list = df['text'].dropna().astype(str).tolist()\n",
    "\n",
    "# 2. Tokenization & Vocab Building\n",
    "# Combine all poems, lowercase them, and split into words\n",
    "all_words = \" \".join(poems_list).lower().split()\n",
    "vocab = sorted(list(set(all_words)))\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 3. Create Sequences (X = word, Y = next word)\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for poem in poems_list:\n",
    "    tokens = poem.lower().split()\n",
    "    for i in range(len(tokens) - 1):\n",
    "        input_seq.append(word_to_idx[tokens[i]])\n",
    "        target_seq.append(word_to_idx[tokens[i+1]])\n",
    "\n",
    "inputs = torch.LongTensor(input_seq)\n",
    "targets = torch.LongTensor(target_seq)\n",
    "\n",
    "print(\"--- Data Preprocessing ---\")\n",
    "print(f\"Total Unique Words (Vocab): {vocab_size}\")\n",
    "print(f\"Total Training Pairs: {len(inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e152e3-f33c-497d-a9aa-ce2c7ff538ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Part 2: One-Hot RNN ---\n",
      "Epoch [20/100], Loss: 7.1405\n",
      "Epoch [40/100], Loss: 6.2363\n",
      "Epoch [60/100], Loss: 5.4967\n",
      "Epoch [80/100], Loss: 4.4459\n",
      "Epoch [100/100], Loss: 3.5648\n"
     ]
    }
   ],
   "source": [
    "class OneHotRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(OneHotRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert index to one-hot on the fly\n",
    "        x_one_hot = nn.functional.one_hot(x, num_classes=self.vocab_size).float()\n",
    "        x_one_hot = x_one_hot.unsqueeze(1) # Add sequence dimension (batch, seq_len, vocab_size)\n",
    "        out, _ = self.rnn(x_one_hot)\n",
    "        out = self.fc(out.squeeze(1))\n",
    "        return out\n",
    "\n",
    "# Training One-Hot Model\n",
    "model_one_hot = OneHotRNN(vocab_size, 64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_one_hot.parameters(), lr=0.005)\n",
    "\n",
    "print(\"\\n--- Training Part 2: One-Hot RNN ---\")\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_one_hot(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15e1756-c009-4bc2-808b-c35b0e8b7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Part 3: Embedding RNN ---\n",
      "Epoch [20/100], Loss: 6.9105\n",
      "Epoch [40/100], Loss: 5.1824\n",
      "Epoch [60/100], Loss: 4.2695\n",
      "Epoch [80/100], Loss: 3.7832\n",
      "Epoch [100/100], Loss: 3.4792\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super(EmbeddingRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x).unsqueeze(1) # (batch, seq_len, embed_dim)\n",
    "        out, _ = self.rnn(x_embed)\n",
    "        out = self.fc(out.squeeze(1))\n",
    "        return out\n",
    "\n",
    "# Training Embedding Model\n",
    "embed_dim = 50\n",
    "model_embed = EmbeddingRNN(vocab_size, embed_dim, 64)\n",
    "optimizer_embed = optim.Adam(model_embed.parameters(), lr=0.005)\n",
    "\n",
    "print(\"\\n--- Training Part 3: Embedding RNN ---\")\n",
    "for epoch in range(100):\n",
    "    optimizer_embed.zero_grad()\n",
    "    outputs = model_embed(inputs)\n",
    "    loss_e = criterion(outputs, targets)\n",
    "    loss_e.backward()\n",
    "    optimizer_embed.step()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss_e.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57527c6c-7d0f-4a44-9986-6cca1b5f143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Analysis & Comparison ---\n",
      "Final One-Hot Loss: 3.5648\n",
      "Final Embedding Loss: 3.4792\n",
      "------------------------------\n",
      "Seed word: 'o'\n",
      "Generated (One-Hot): o lord, to the young men and the young\n",
      "Generated (Embedding): o lord, i am to the young men and\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_word, length=8):\n",
    "    model.eval()\n",
    "    word = start_word.lower()\n",
    "    if word not in word_to_idx: return \"Word not in vocab\"\n",
    "    \n",
    "    result = [word]\n",
    "    for _ in range(length):\n",
    "        inp = torch.LongTensor([word_to_idx[word]])\n",
    "        out = model(inp)\n",
    "        _, next_idx = torch.max(out, dim=1)\n",
    "        word = idx_to_word[next_idx.item()]\n",
    "        result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Pick a seed word from your data\n",
    "seed = all_words[0]\n",
    "\n",
    "print(\"\\n--- Final Analysis & Comparison ---\")\n",
    "print(f\"Final One-Hot Loss: {loss.item():.4f}\")\n",
    "print(f\"Final Embedding Loss: {loss_e.item():.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Seed word: '{seed}'\")\n",
    "print(\"Generated (One-Hot):\", generate_text(model_one_hot, seed))\n",
    "print(\"Generated (Embedding):\", generate_text(model_embed, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b08eab-2287-4ca7-88a0-0bc694e27067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
